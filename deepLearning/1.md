## 第一份工作的重点
前提是已经有了EFSM的状态机流程图，是条件可利用
状态机图
遍历其中可能出现的路径
**寻找其中代价最小的路径**
可能路径是无法走通的
因为EFSM的状态机不仅有输入和输出，还有必要的条件，通过？作为输入，！作为输出
但是路径上的的条件可能是矛盾的，这导致了有些路径实质无法走通，虽然看起来是通路

## 可能需要避免生成数据冲突的测试序列
## 在EFSM的定义中n_actions是变迁的动作，n_features是指的是状态空间里面的变量个数
## 具体的算法构建做法(考虑改变的DQN+蚁群算法)
1. 通过首先实现强化学习模型，拿到模型权重参数
2. 构建EFSM类，在其中声明算法模型，并加载算法权重参数文件
3. EFSM类中关键实现在于computer distance \
其中ret_val是环境的历史路径(self.env.history_path)，以及环境内部迭代的参数(attempts(上下文变量), password(pin), service_balance(sb), client_balance(cb))
## 蚁群算法：
需要知道几个重点
1. 信息素图，默认初始化每个点的信息素为2
蚂蚁留下信息素是在经过的路径(start, end)边上，需要edge矩阵, 
构建一个全0的矩阵计算delta信息素
添加时，由于信息素会挥发，所以在原来的基础上*a即挥发率
```python
## temp为变化量，另一个是原来的
    for i in range(city_num):
        for j in range(city_num):
            pheromone_graph[i][j] = pheromone_graph[i][j] * RHO + temp_pheromone[i][j]
```
2. 如何选择，由于有了已经训练完毕的强化学习模型，所以ant from cityA to cityB 的step可以轻易获得
通过step的长度和信息素共同决定蚂蚁到下个城市的概率，同时将概率累加，
通过轮盘堵转法决定要去的城市，即按序减去城市的概率，当减去城市概率要<=0时即是目标城市。


## 
本文中使用的DRL算法是DQN，通过存储statei (当前状态), statej (采取动作后状态), actioni, rewardi,将这4个变量作为算法执行存储的节点。在DQN算法中存在一个记忆库的结构，用于存储过去智能体行动的节点信息。DQN算法需要在记忆库已经存储了部分内容之后才可以启动学习。DQN网络的构建中采用了2个网络，eval_net用于生成Q_eval(Q估计参数), target_net用于生成Q_next(Q下一步参数)。
当DQN算法启动学习时，先复制一份eval_net的参数到target_net的参数。通过对于记忆库内记忆节点的计数cnt，随机产生cnt个索引，通过这些索引构成一批记忆。每次learn将Q_eval复制给Q_target，由Q_next和Q_eval以及记忆节点中采取对应action获得的reward信息共同更新Q_target. 最后由这份Q_target反向传播计算loss,调整优化eval_net参数.
在算法模型参数训练完成之后，本文所提到的DQN网络已经具备了在接受对应观察状态时，采取对应action，选择rewardmax的下一个节点的能力。在DQN模型和蚁群算法的结合过程中，由于蚁群算法中一步关键的实现在于为蚂蚁选择下一个应当前往的节点，这一步决策可以由DQN模型完成。DQN模型给出蚂蚁从当前节点到下一个节点的路径，蚁群算法根据信息素浓度和算法模型给出的路径长度计算该选择的概率，如公式1所示，同时要求DQN所去的目标节点，不应该是已经遍历过的。合计上述城市的概率，通过轮盘赌转算法得到下一个城市的序号，同时更新全图的信息素浓度。

## 
为了使EFSM的模型环境更适合DQN模型的训练，采用图的结构来描述EFSM模型的环境。 构造两个二维矩阵，第一个矩阵是距离矩阵，用来描述EFSM模型的图结构，更准确的说是奖励矩阵，其中行索引代表当前状态或者选择后的状态 过渡，列索引表示过渡的目标状态。 第二个矩阵是过渡节点选择矩阵。 每行的前n个数据表示可选的转移节点（所有不可选的都设置为-1），最后一个数据表示可选动作的个数。 DQN模型最终生成选择的动作，环境env根据动作返回动作后的状态next_state，奖励reward，是否找到目标结束转移target_tran。


##
In this sections, we discuss the main ideas behind our deep reinforcement learning and ant colony algorithm. Finding a test sequence that satisfies a given coverage criterion can be regarded as a combinatorial optimization problem similar to TSP. Specifically, the transition set TTS to be covered in the coverage criterion is regarded as a group of cities in the TSP, and generating a test sequence that satisfies the coverage criterion is equivalent to finding a shortest path through all the transitions in the TTS. When TTS = [], the test sequence generation problem can be described as a connected graph (V, E). In the Graph , the nodes set V =  express the state sets, the edges set E express the path length that between node ti and node tj (  This is similar to the connected graph  describing the TSP. Therefore, the main idea of TSG-DA is to transform the test sequence generation problem into a combinatorial optimization problem similar to TSP, and use ACO to solve it.
However, there are two differences between the EFSM test sequence generation problem and the TSP. On the one hand, the initial node of  must be a transition from the initial state, and after finding a shortest path that meets the coverage criterion, a search is completed without returning to the initial node. If there is only one transition starting from the initial state in an EFSM, the starting node of each path search is this transition. In particular, the coverage criterion requires that none of the covered transitions may start from the initial state, and at this time, it is necessary to randomly add a starting node to . On the other hand, since the shortest executable path between transitions in EFSM may change as the context variable changes, the value of the edge in  is the dynamic path length, while the value of the edge in  is static city distance. 
When the DQN algorithm starts learning, it first copies the parameters of eval_net to the parameters of target_net. By counting cnt of memory nodes in the memory bank, cnt indexes are randomly generated, and a batch of memories are formed through these indexes. Each time learn copies Q_eval to Q_target, and Q_target is updated by Q_next and Q_eval together with the reward information obtained from taking the corresponding action in the memory node. Finally, the loss is calculated by backpropagation of this Q_target, and the eval_net parameters are adjusted and optimized.
After the algorithm model parameters are trained, the DQN network mentioned in this paper has the ability to take the corresponding action and select the next node of rewardmax when receiving the corresponding observation state. In the process of combining DQN model and ant colony algorithm, because the key step of ant colony algorithm is to select the next node that ants should go to, this step decision can be completed by DQN model. The DQN model gives the path of the ant from the current node to the next node, and the ant colony algorithm calculates the probability of this choice according to the pheromone concentration and the path length given by the algorithm model, as shown in Equation 1. At the same time, the target node that DQN goes to should not have been traversed. The probability of the above cities is aggregated, and the sequence number of the next city is obtained by the roulette wheel rotation algorithm, and the pheromone concentration of the whole map is updated at the same time.
