## 确定tensorflow compile函数的作用
q_net 进行了编译构建
target_net并没有
但是都可以执行predict操作
需要确定compile函数的行为
compile：只是配置必要的运行参数，如优化器和loss计算，并没有编译模型
predict: 会调用model的call()
train_on_batch / fit: 反向传播调整参数

# 我们的trans_matrix和我们的actions返回是否一致
我们取了所有状态下可选择变迁数量最大的作为n_actions
通过 % feasible_actions得到选择变迁的索引

# 形式化奖励
目前的问题是在如何设定较为合理的奖励函数去领导机器人遍历变迁
机器人只能遍历我们提供的可能的变迁，利用取余等做法来完成。
目前可能的思路是：
1. 即使图的边权上有许多的条件，但仍能通过某些方法避免先考虑
2. 在HalfCheetah环境中，我们有一个受限于一个垂直平面的双足机器人，这意味着它只能向前或向后运动。
机器人的目标是学会跑步步态，奖励是速度。这是一种形式化的奖励，机器人越接近奖励目标，系统给予的奖励就越多。这和稀疏奖励形成鲜明对比，因为这种奖励只在目标状态下给予奖励，在其他任何地方则没有奖励。这种形式化的奖励通常更容易促进学习，因为即使策略没有找到解决问题的完整解决方案，它们也能提供积极的反馈。具体的关于halfcheeath的原理如图:它是一个多自由度的控制,具体的是每个自由度上的速度,而计算奖励则是按照前后位置的位置进行计算.

# 奖励机制的设计
应当使得奖励矩阵以及相应的机制具备一定的引导能力
应当有一个主要的奖励设置去引导(主要奖励)，而其他奖励可以通过 / factor减小其影响(不是那么重要的)
1. 目标导向：奖励应该与任务目标紧密相关，以引导智能体朝着目标方向学习。例如，如果目标是让机器人在最短时间内到达某个目标地点，可以设置奖励为到达目标地点时的正奖励，同时在每一步给予一定的负奖励，以激励智能体尽快达成目标。
2. 简单明确：尽可能设置简单、明确且易于理解的奖励。过于复杂的奖励设置可能导致智能体难以学习到有效的策略。避免使用过多条件和边界情况，以降低智能体学习的难度。
3. 稀疏与密集奖励：根据任务的特点和难度，选择适当的奖励密度。稀疏奖励（sparse reward）意味着智能体在很长一段时间内可能无法获得任何奖励，这可能导致学习速度变慢。密集奖励（dense reward）为智能体提供了更多的学习信号，但有时可能导致局部最优解。在实际应用中，可以尝试在稀疏和密集奖励之间寻找平衡。
4. 避免负面激励：谨慎设置负奖励，以避免负面激励。例如，在机器人导航任务中，仅给予碰撞时的负奖励可能会导致机器人过于谨慎，从而影响性能。在这种情况下，可以尝试调整奖励结构，增加对正确行为的激励。
5. 归一化奖励：为了让学习算法更稳定，可以将奖励值归一化到一个较小的范围内，例如[-1, 1]。这有助于智能体更容易地学习和比较不同行为的效果。
6. 调试与调整：设置奖励时，可能需要反复试验和调整。



## 考虑抽象一下咱们的动作
在机器人路径规划中，设计前后左右4种动作还有原地打转1种这些是强化学习指定的
**但是还有一种动作，保底动作通过A*算法寻路的动作** -------------接下来实现的重点
在我们原本的设计中，设计了一个概率可以自由探索外域，防止局部最优解
但是，可以预先通过传统的路径规划算法ACO, A*进行路径引导

## 需要立刻实现的工作，看一下A*算法尝试通过传统的路径搜索算法给出一个动作
因为我们希望直接由RL生成路径，根据分治的思想，一个端到端的路径可以分解为n个端到端的路径组成
动作集合最大为n_actions为可选择的变迁中的最大值 + 1 (其中a_0为保底变迁采用路径规划算法给出动作)
action_generate %= feasible_actions, 我们轻松可以得到feasible_actions

如果我们希望使用Astar algorithm 我们需要实现相应的grid
## 上述说法当做后面优化的点
现在急切需要解决的问题是
1. states 可能需要大改设计, 但是仍然保留context_vars, 除非有可以对context_vars 进行综合考虑的函数
或者定义
2. we use double RL core, the first core pay attention to the shortest path from tran to tran 
    and the second RL core pay attention to the order between trans.

修改查找算法设计, 因为在robot大量探索的路上可以路过需求的点，这就算find了
那么重新设置target_tran还有action即可

1. preprocess the begin_transition to the end_transition for the first RL core
2. in this situation it is easy to find the road
3. during the generate the whole test sequence use the second RL core
4. 或许奖励不应该设置为带有极强的指向性的, 因为可能合适的指向性得不到想要的效果
(这和EFSM图结构内路径的各种条件的耦合有关)
需要考虑自环带来的影响，优先处理无法缩短branch_distance的自环，给予penalty

处理端到端的RL_core需要长时间的迭代才能得到稳定的结果, 好找一些的路径目测在100轮就可以完成
不好找的路径可能要500轮以上, 且没有较为合适的普遍化做法。

问题在于context_vars变量组的维护有问题, 建议综合考量多个模型后再训


**由于群体智能算法的需求, 在假装step env时需要考虑, 复制一份当前环境**
```python
#---------------
copy_env = copy.deepcopy(self.env)
....
# after some code handle

#---------------
```



